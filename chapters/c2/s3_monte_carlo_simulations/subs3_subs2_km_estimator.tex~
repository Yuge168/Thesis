\subsubsection{Kaplan-Meier Estimator}

The general definition of the survival time is the time starting from
a specified point to the occurrence of a given event
\cite{bewick2004statistics}, such as death, pregnancy, job loss,
etc. Also, the analysis of the group of survival data is called
survival analysis \cite{altman1990practical}. In the survival
analysis, three kinds of situations will affect the subjects' survival
time \cite{goel2010understanding}. Firstly, the subjects are
uncooperative and refused to continue to participate in the
research. Secondly, some subjects do not experience the event before
the end of the study, but they would have experienced the event if
they keep being observed. Finally, the researchers lose touch with the
subjects in the middle of the investigation. In practice, since these
subjects have partial information about survival, the scientists will
label the above circumstances as censored observations
\cite{bewick2004statistics} instead of ignoring them and decreasing
sample size. 

In clinical trials or community trials, Kaplan-Meier Estimator
\cite{kaplan1958nonparametric}, a non-parametric analysis, is a
commonly applied statistical method in the survival analysis for the
measurement of the fraction of the survival time after the treatment
\cite{aalen2008survival} and for generating the corresponding survival
curve. It also works well with the mentioned three difficult
situations. With various assumptions \cite{etikan2017kaplan}
\cite{goel2010understanding}, the Kaplan-Meier survival curve can be
created and provides the probability of surviving in a given length of
time while considering the time in many small intervals
\cite{altman1990practical}.


The output of random walk models is particles' wandering time, also
defined as the number of steps, before encountering the absorbing
boundary. Particle's behaviours are affected by the boundary
conditions, and each of them carries some partial geometrical
information of the edges. In this thesis, the numerical survival
functions, which can be estimated by \cite{aalen2008survival}

\begin{equation}
  \widehat{S}(t) = \prod_{i:t_i \leq t} (1 - \frac{d_i}{n_i})
\end{equation}

where $t_i$ is a time when at least one particle reaches the absorbing
boundary, $d_i$ the number of particles encountering with the target
boundary at time $t_i$, and $n_i$ is the number of particles which
have not yet stopped moving up to time $t_i$.


\subsection{Sample Size Determination}

Based on the law of large number (LLN) \cite{dekking2005modern}, as
the sample size approaches infinity, the sample mean tends to get
closer to the true population mean with the high
probability. Moreover, the central limit theorem (CLT)
\cite{dekking2005modern} illustrates how the sampling distribution of
the mean changes as a function of the sample size and how much more
reliable a large experiment is. On the downside, the increasing number
of trials results in the higher cost of performing the simulation,
which is the major drawback of the fixed time-step Monte Carlo
simulations. Therefore, it is necessary to conduct the minimum number
of simulation runs to achieve a desired degree of precision.
