\section{Monte Carlo Simulations}

In this section, several widely used numerical methods for solving
partial differential equations (PDEs), such as the diffusion equation,
and their limitations in practice are introduced. From the
deterministic perspective, one of the probabilistic algorithms,
Monte-Carlo (CM) methods, is proposed. However, discretization errors
also affect the accuracy of the solutions. Finally, based on the
theories of two random walk models, Pearson's random walks (PRWs) and
lattice random walks (LRWs), two fixed-time step Monte Carlo
simulations are designed. Those simulations can approximate the
integration of the heat equations' solutions, named survival
functions, which describes the geometrical properties of the shape.


\subsection{Background}

In the last section in this chapter, the exact analytical solutions of
the heat equation defined in the annulus with the initial and boundary
conditions have been derived. And then, the analytical survival
probability can be calculated by the integration over the annulus for
shape characterization. This analytical method looks useful, but its
applications to practical problems will present difficulties. Firstly,
the numerical evaluation of the analytical solutions is usually by no
means trivial because they are in the form of infinite
series. Secondly, either irregular geometries or discontinuities cause
the complication in solving the heat (diffusion) equations by
analytical methods in practice, so the explicit algebraic solutions
are close to non-existed. In other words, the analytical techniques
and solutions have a severe limitation that they can apply strictly
only to the linear form of the diffusion equations and the boundary
conditions \cite{crank1979mathematics}. Therefore, numerical methods
and computer simulations are more useful and accessible in solving
differential equations than calculating pure analytical solutions.

\subsubsection{Deterministic Numerical Methods}

The techniques for solving initial-boundary value problems (IBVPs)
based on numerical approximations have existed for a long time. The
finite-difference methods (FDM) are frequently used and easily
implemented in solving the differential equations by converting them
into a system of algebraically solvable equations
\cite{grossmann2007numerical}. The basic idea is to replace the
derivatives in the equations by difference quotients. For example, the
FTCS (Forward Time Centered Space) scheme
\cite{pletcher2012computational} aims to approximate the numerical
solutions of the heat equation and other similar parabolic PDEs. FTCS
discretizes the Laplace operator in space and the time derivative and
implements boundary conditions on the staggered grid to represent the
original continuous problem, but it is numerically stable if and only
if it satisfies a specified condition
\cite{pletcher2012computational}. Generally, there are two classes of
errors appearing in FDM called round-off error and truncation
error. The former one results from the loss of precision due to the
computer rounding of decimal quantities. The latter one is caused by
discarding the remainders in the difference approximation and highly
depends on the time and space step. In other words, the smaller the
step size, the longer the simulation duration and higher data quality
\cite{hoffman2018numerical}.


Compared with FDM, the finite element method (FEM) is particularly
solving PDEs with relatively higher quality when the problems are
defined in two or three space dimensions. FEM divides the physical
system's complicated geometries and boundaries into a certain amount
of smaller and simpler subdomains \cite{logan2011first} (eg. lattice,
triangle, curvilinear polygons, etc.). Every subdomain is locally
approximated by a finite set of element equations (piecewise shape
functions) assembled into a larger system of equations for modelling
the entire problem finally \cite{logan2011first}. The goal of FEM is
to approximate a stable solution by minimizing the associated error
function with the variational calculus \cite{logan2011first}. The
significant advantages of FEM are representing the complex geometries
accurately, exploring the local characters of the approximation, and
expressing the solutions in a unified formulation
\cite{reddy1993introduction}. However, FEM needs an amount of
mathematical skill that requires human involvement, such as converting
the original equation into the equivalent weak formulation, choosing
and changing the variational formulation and discretization strategy
in a particular problem, etc. In this thesis, the heat equations
defined in $2$-dimensional images with millions of pixels, the
extremely complex root systems and various boundary conditions, which
makes it time-consuming and challenging to trace and identify the
problems' geometries, label the nodes, and generate the coordinates
and connectivities among the nodes in the preprocessing stage of FEM.


In contrast, the finite volume method (FVM) converts the
$2$-dimensional diffusion equations into a linear equations system,
which can be solved by the direct method
\cite{eymard2000finite}. However, the accuracy of FVM is related to
the integration with respect to time and space. Unlike the domain-type
methods (e.g. FDM, FEM, FVM, etc.), the boundary element method (BEM)
is an alternative numerical computational technique for solving PDEs
formulated as an integral equation based on the boundary. Especially,
when the domain extends to infinity or the boundary is complex, BEM is
more efficient in computation than other methods because of the
smaller surface or volume ratio \cite{katsikadelis2002boundary} since
it only discretizes the boundary and fits the boundary values into the
integral equation \cite{ang2007beginner}.


However, all of the mentioned numerical methods have an intrinsically
similar feature - mesh discretization in the time and space
dimension. They need to deal with the problem in defining the
extremely complicated boundary of the root systems. Moreover, if the
mesh becomes finer and the internal points number becomes larger, the
solutions of the discretized problem will converge to the original
IBVPs with a higher level of accuracy, but the computational time will
be longer. Furthermore, after applying the numerical methods, it will
sometimes take a much longer time to decide whether the solution is
right and adjust the schemes by understanding the systems
qualitatively. More importantly, the extra efforts are demanded based
on the approximated solutions of heat equations since the aimed
numerical approximation is the survival function.



%%%%%_____________________________________________________________%%%%%

\subsubsection{\textcolor{red}{\textbf{\hl{Monte Carlo Techniques}}}}


Monte Carlo (MC) methods, commonly used computational techniques, aim
to generate samples from a given probability distribution, estimate
the functions' expectations under this distribution, and optimize the
complicated objective functions by using random numbers
\cite{kroese2014monte}. Compare with the deterministic numerical
methods, applying Monte Carlo methods for solving PDEs is grid-free on
the domain, boundary, and the boundary conditions of the problem
 \cite{grebenkov2014efficient}.

From the deterministic perspective, sloving PDEs by Monte Carlo
procedure is based on the classical probabilistic representations,
e.g. Feynman-Kac representations \cite{kac1987enigmas}, in the form of
Wiener or diffusion path integral \cite{sabelfeld2013random}. For
example, when solving the $d$-dimensional heat equation, the Brownian
motion generated by the second-order differential operator is
simulated using the numerical methods for solving the stochastic
differential equation \cite{wiki:MCMheat}. However, the discretization
step should be small enough to achieve the desired accuracy, which
will result in the long simulation.

Another straightforward approach for solving PDEs by Monte Carlo
method based on the probabilistic interpretation of integral equations
equivalent to the original continuous problem, whose solutions can be
represented as expectation of some functions of the trajectories of a
stochastic process
\cite{varadhan1980lectures}\cite{grebenkov2014efficient}\cite{sabelfeld2013random}.
Those trajectories generated by Monte Carlo techniques approximate the
expectations and the solutions. For instance, the Markov process is
proposed for estimating the expectations by simulating the successive
positions of the trajectory of the process at fixed instants with step
$\triangle t$ \cite{kronberg1976solution}\cite{king1951monte}.

%%%%%_____________________________________________________________%%%%%


\subsection{Models for Continious Diffuison Process}

Models, based on the reasoning with the averaged physical quantities,
usually lead to a diffusion equation, including concentration,
temperature, and velocity. These quantities at a space-time point
represent averages around the point in a small time interval and
spatial volume. Compared to modelling experimental and practical
situations using diffusion equations and solutions, random walk is
an entirely distinct and simplified kind of modelling procedure. It
simulates the underlying physical processes that involve the
complicated microscopic movement of atoms and molecules, in a simple
and computationally efficient way. Moreover, when averaged, the random
walk generates models that are mathematically equivalent to heat
(diffusion) equations.

\subsubsection{Borwnian Motion and Random Walks}

The heat equation describes the temperature distribution of a certain
homogeneous and isotropic domain that does not exist any heat sources
\cite{varadhan1980lectures}. In the domain, the heat spreads randomly
in all directions at some rate, so it is easier to understand
diffusion (heat) equations by considering the heat as the individual
random particle \cite{lawler2010random}. The spreading of the heat
'particles' is a diffusion process defined in continuous time with a
continuous state space and continuous sample paths
\cite{ito2012diffusion}. Brownian motion
\cite{brown1828microscopical}, also called the Wiener process, is a
case of a diffusion process with the Markov properties: future states
depend only on the present states \cite{bharucha2012elements}. From a
probabilistic perspective, the probability density of Brownian
particles satisfies the heat (diffusion) equation
\cite{kac1947random}\cite{varadhan1980lectures}.



Brownian motion \cite{brown1828microscopical}, the irregular motion of
individual pollen particles, has been existed for a long time before
the random-walk theory. At the beginning of the twentieth century, the
term, random walk, was initially proposed by Karl Pearson
\cite{pearson1905problem}. He described a simple example of the
isotropic planar random flights to model how mosquitoes migrate and
invade randomly in the cleared jungle regions. At each time step, the
mosquito moves to a random direction with a fixed step
length. Rayleigh \cite{rayleigh1905problem} stated that Pearson's
question, the distributions of mosquitos after many steps have been
taken, is the same as superpositioning the sound vibrations with unit
amplitude and arbitrary phase \cite{de2012flying}.


At almost the same time, Louis Bachelier \cite{bachelier1900theorie}
developed a model of the financial time series based on the random
walk and explored the connection between discrete random walks and
continuous diffusion (heat) equation. During the development of random
walk theory, many important fields, such as random processes, random
noise, spectral analysis, and stochastic equations, were developed by
some physicists \cite{einstein1905electrodynamics}
\cite{einstein1906theory} \cite{smoluchowski1916drei}. The continuous
Brownian motion can be considered as the limit of discrete random walk
as the time and space increments go to zero \cite{lawler2010random}.





\subsubsection{Theory of Lattice Random Walks (LRWs)}

Let us consider the heat particles perform the simple random walk on
the $d$-dimensional integer grid $\mathbb{Z}^d$. It is a
discrete-space and discrete-time symmetric hopping process
\cite{redner2001guide} on the lattice. At each time step, a random
walker moves to one of its $2d$ nearest neighbours with probability
$\frac{1}{2d}$. If $d \leq 2$, the random walk is recurrent, in which
the particle will returns to its origin infinitely often with the
probability $1$. If $d \geq 3$, the random walk is transient in which
the particle will return to its origin only finitely often with
probability $1$ \cite{hughes1998random}.


For simplicity, we start from considering $2$-dimensional lattice
random walks (LRWs) \cite{lawler2010random}. Let $\triangle l$ be the
distance between two sites and $\delta$ be the time step. Define $p(x,
t| x_{S})$ as the conditional probability of a particle to be in
position $x$ at time $t$ starting from $x_{S}$ at time $t=0$. The
temperature at a site is given by the amonunt of heat going in from
nerighboring site. Thus, $p(x, t + \delta |x_{S})$ is defined as
probability of a particle to be in position $x$ at time $t + \delta$
starting from $x_{S}$ at time $t=0$,

\begin{equation}
  p(x, t + \delta | x_{S}) = \frac{p(x - \triangle l e_{x}, t| x_{S}) +
    p(x + \triangle l e_{x}, t| x_{S}) + p(x - \triangle l e_{y}, t|
    x_{S}) + p(x + \triangle l e_{y}, t| x_{S})}{4}
\end{equation}

where $e_x$ and $e_y$ are unit vectors of $x-$ axis and $y-$ axis, respectively. 


When $\delta \rightarrow 0$, $\triangle l \rightarrow 0$, and $\delta \sim (\triangle l)^2$, 

\begin{equation}
  p(x, t | x_{S}) + \delta p(x, t | x_{S}) = p(x, t | x_{S}) +
  \frac{(\triangle l)^2}{4} (\frac{\partial ^2 p(x, t|
    x_{S})}{\partial x^2} + \frac{\partial^2 p (x, t| x_{S})}{\partial
    y^2})
\end{equation}

Finally, the $2$-dimensional heat equation is

\begin{equation}
  \frac{\partial p(x, t| x_{S})}{\partial t} = \frac{(\triangle
    l)^2}{4 \delta} (\frac{\partial ^2 p(x, t| x_{S})}{\partial x^2} +
  \frac{\partial^2 p (x, t| x_{S})}{\partial y^2})
\end{equation}
where $D = \frac{(\triangle l)^2}{4 \delta}$ is the diffusion
coefficent. This derivation shows a tight relationship between
lattice random walks and diffusion.


\subsubsection{Theory of Pearson's Random Walks (PRWs)}

Based on Pearson's problem and Rayleigh's answer, Stadje
\cite{stadje1987exact} and Masoliver et al. \cite{masoliver1993some}
considered a two-dimensional continuous-time and continuous-space
random walk, defined as Pearson's random walks (PRWs) in this thesis,
moving with constant speed and with random directions distributed
uniformly in $[0, 2\pi)$. Moreover, in PRWs, the lengths of the
  straight-line paths and the turn angles are stochastically
  independent. If the mean step length approaches zero and the time is
  big enough, the behaviours of particles in PRWs weakly converges to
  the Wiener Process \cite{stadje1987exact}, which satisfies the
  traditional heat equation.
  
  
\subsection{Output Analysis}

\subsubsection{Relationship between $t$ and $\tau$}

Particles' average one-step displacement $\triangle l$ in the
fixed-time step Monte Carlo simulations, LRWs and PRWs, are associated
with the time step is $\delta$:

\begin{equation}
  \triangle l = 2 \sqrt{D \delta}
\end{equation}
where $D$ is the diffusion coefficent.

Eq.$(2.15)$ implies that the time step $\delta$ must be designed small
enough to make sure that $\triangle l$ is shorter than the smallest
geometrical features of the boundaries. Thus, $\triangle l$ should
equal or be less than one-pixel size in the simulations. Furthermore,
the $\delta$ is regarded as a fundamental bridge between particles'
number of steps $t$ and unitless continuous-time $\tau$,

\begin{equation}
 \tau = t \delta = \frac{(\triangle l)^2 t}{4D}
\end{equation}
where $D$ is 1.

When running the LRWs in the annulus, $\triangle l$ is always
$\frac{1}{100}$ since particle's step length is as same as one-pixel
size. However, $\triangle l$ is related to the particle's step length
in PRWs. If particle's step length is $0.5$, a half of a pixel, then
$\triangle l$ equals $\frac{1}{100} \times \frac{1}{2} =
\frac{1}{200}$.  Similiarly, when the step length in PRWs is $0.1$,
then $\triangle l$ is $\frac{1}{1000}$. 


\subsubsection{Kaplan-Meier Estimator}

The general definition of the survival time is the time starting from
a specified point to the occurrence of a given event
\cite{bewick2004statistics}, such as death, pregnancy, job loss,
etc. Also, the analysis of the group of survival data is called
survival analysis \cite{altman1990practical}. In the survival
analysis, three kinds of situations will affect the subjects' survival
time \cite{goel2010understanding}. Firstly, the subjects are
uncooperative and refused to continue to participate in the
research. Secondly, some subjects do not experience the event before
the end of the study, but they would have experienced the event if
they keep being observed. Finally, the researchers lose touch with the
subjects in the middle of the investigation. In practice, since these
subjects have partial information about survival, the scientists will
label the above circumstances as censored observations
\cite{bewick2004statistics} instead of ignoring them and decreasing
sample size. 

In clinical trials or community trials, Kaplan-Meier Estimator
\cite{kaplan1958nonparametric}, a non-parametric analysis, is a
commonly applied statistical method in the survival analysis for the
measurement of the fraction of the survival time after the treatment
\cite{aalen2008survival} and for generating the corresponding survival
curve. It also works well with the mentioned three difficult
situations. With various assumptions \cite{etikan2017kaplan}
\cite{goel2010understanding}, the Kaplan-Meier survival curve can be
created and provides the probability of surviving in a given length of
time while considering the time in many small intervals
\cite{altman1990practical}.


The output of random walk models is particles' wandering time, also
defined as the number of steps, before encountering the absorbing
boundary. Particle's behaviours are affected by the boundary
conditions, and each of them carries some partial geometrical
information of the edges. In this thesis, the numerical survival
functions, which can be estimated by \cite{aalen2008survival}

\begin{equation}
  \widehat{S}(t) = \prod_{i:t_i \leq t} (1 - \frac{d_i}{n_i})
\end{equation}

where $t_i$ is a time when at least one particle reaches the absorbing
boundary, $d_i$ the number of particles encountering with the target
boundary at time $t_i$, and $n_i$ is the number of particles which
have not yet stopped moving up to time $t_i$.


\subsection{Sample Size Determination}

Based on the law of large number (LLN) \cite{dekking2005modern}, as
the sample size approaches infinity, the sample mean tends to get
closer to the true population mean with the high
probability. Moreover, the central limit theorem (CLT)
\cite{dekking2005modern} illustrates how the sampling distribution of
the mean changes as a function of the sample size and how much more
reliable a large experiment is. On the downside, the increasing number
of trials results in the higher cost of performing the simulation,
which is the major drawback of the fixed time-step Monte Carlo
simulations. Therefore, it is necessary to conduct the minimum number
of simulation runs to achieve a desired degree of precision.


\subsubsection{General Method}

There are five parts of the standard way to determine the sample size
in the Monte Carlo simulations. Firstly, simulating with a certain
amount of samples. Secondly, repeating the simulation several times
with the sample size as the same as the first step.  Thirdly,
increasing the number of samples and implementing the first two steps
again. Fourthly, running a regression analysis of the variability of
the sample statistic as a function of sample size. Finally, estimating
the sample size that will result in any desired level of convergence
by some probabilistic inequalities, including Chebyshev's inequality
\cite{chebyshev1867valeurs}, Cantelli’s inequality
\cite{cantelli1929sui}, Vysochanskij–Petunin inequality
\cite{vysochanskij1980justification}, etc.


\subsubsection{Dvoretzky–Kiefer–Wolfowitz (DKW) inequality}

Although the sample size estimated by the general method does not
depend on the geometries characterized by the random walk models, how
long the simulation will run is unknown, and the final calculated
sample size will be larger than the really necessary value
sometimes. Therefore, an alternative method, named the
Dvoretzky–Kiefer–Wolfowitz (DKW) inequality
\cite{dvoretzky1956asymptotic}, is proposed for the sample size
determination without simulating random walk models and considering
the shape of objects in the images.

Let $F_N(x)$ denote the empirical distribution functions (empirical
CDF) for a sample of $N$ real-valued $i.i.d.$ random variables,
$X_{1}, ... , X_{N}$, with continuous cumulative distrbution function
(CDF) $F(x)$. The DKW inequality, as expressed in Eq.$(2.18)$, bounds the
probability that the random function $F_{N}(x)$ differs from the true
$F(x)$ by more than a given constant $\varepsilon$
\cite{dvoretzky1956asymptotic}.

\begin{equation}
  Pr(\sup_{x \in \mathbb{R}} |F_{N}(x) - F(x)| > \varepsilon) \leq
  2e^{-2N\varepsilon^2} \;\; \;\; for \; every \; \varepsilon > 0
\end{equation}

The equally spaced confidence bounds or simultaneous band around the
$F_{N}$ encompassing the entire $F(x)$ can be expressed by
\begin{equation}
  F_{N}(x) - \varepsilon \leq F(x) \leq F_N(x) + \varepsilon \; \; \; \; 
\end{equation}

On the other hand, assume the simultaneous band produced by Eq.$(2.19)$
containing the $F(x)$ at a given confidence level $1-\alpha$, the
interval $\varepsilon$ can be calculated by

\begin{equation}
  \varepsilon = \sqrt{\frac{\ln{\frac{2}{\alpha}}}{2N}}
\end{equation}


Given a converge probability $\alpha$ and constant $\varepsilon$, it is
straightforward to estimate the sample size $N$ in the fixed-time step
Monte Carlo simulations in any images by Eq.$(2.20)$.

